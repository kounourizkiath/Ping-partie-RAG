import os
import shutil
from glob import glob
import warnings

# Filtrage des avertissements de parsing PDF pour nettoyer la sortie du terminal
warnings.filterwarnings("ignore", category=UserWarning, module='pypdf') 

# Importations LangChain mises √† jour
from langchain_community.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# ==============================================================================
# 1. CONFIGURATION DU PROJET
# ==============================================================================

# Le r√©pertoire racine contenant tous les documents √† ing√©rer (PDF, TXT, etc.)
DATA_DIR = "data"

# Le r√©pertoire o√π ChromaDB stockera physiquement les vecteurs.
CHROMA_DB_DIR = "vectordb"

# Le nom de la collection (l'√©quivalent d'une table) dans ChromaDB.
COLLECTION_NAME = "secops_documentation_ping56"

# Mod√®le d'embedding choisi pour la vectorisation.
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# Extensions de fichiers support√©es
SUPPORTED_EXTENSIONS = ['.pdf', '.txt']

# ==============================================================================
# 2. PARAM√àTRES DE CHUNKING (MEILLEURES PRATIQUES RAG)
# ==============================================================================

# Taille maximale de chaque morceau de texte (chunk) en nombre de caract√®res.
CHUNK_SIZE = 1000

# Nombre de caract√®res de chevauchement entre deux chunks cons√©cutifs.
CHUNK_OVERLAP = 200

# ==============================================================================
# 3. FONCTIONS PRINCIPALES DU PIPELINE
# ==============================================================================

def clean_and_prepare_db():
    """
    Fonction de maintenance.
    Supprime le r√©pertoire de la base de donn√©es Chroma existante pour garantir
    une nouvelle ingestion propre et √† jour.
    """
    if os.path.exists(CHROMA_DB_DIR):
        print(f"üóëÔ∏è  Suppression de l'ancienne base de donn√©es √† : {CHROMA_DB_DIR}")
        shutil.rmtree(CHROMA_DB_DIR)
    print("‚úÖ R√©pertoire de la base de donn√©es pr√™t.\n")

def load_all_documents():
    """
    Fonction de chargement am√©lior√©e.
    Recherche et charge tous les fichiers PDF et TXT trouv√©s r√©cursivement dans le dossier DATA_DIR.
    """
    all_documents = []
    stats = {"pdf": 0, "txt": 0, "errors": 0}
    
    # Recherche de tous les fichiers PDF et TXT
    pdf_files = glob(os.path.join(DATA_DIR, "**", "*.pdf"), recursive=True)
    txt_files = glob(os.path.join(DATA_DIR, "**", "*.txt"), recursive=True)
    
    total_files = len(pdf_files) + len(txt_files)
    
    if total_files == 0:
        print(f"‚ö†Ô∏è  ATTENTION : Aucun fichier PDF ou TXT trouv√© dans le r√©pertoire '{DATA_DIR}'.")
        return [], stats

    print(f"üìö {len(pdf_files)} fichiers PDF trouv√©s")
    print(f"üìÑ {len(txt_files)} fichiers TXT trouv√©s")
    print(f"üìä Total : {total_files} documents √† charger\n")
    
    # Chargement des fichiers PDF
    print("üîÑ Chargement des PDFs...")
    for file_path in pdf_files:
        try:
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            all_documents.extend(docs)
            stats["pdf"] += 1
            print(f"  ‚úì {os.path.basename(file_path)} ({len(docs)} pages)")
        except Exception as e:
            stats["errors"] += 1
            print(f"  ‚úó ERREUR {os.path.basename(file_path)}: {str(e)[:80]}")
    
    # Chargement des fichiers TXT
    print("\nüîÑ Chargement des fichiers TXT...")
    for file_path in txt_files:
        try:
            # Utilisation de TextLoader avec encodage UTF-8
            loader = TextLoader(file_path, encoding='utf-8')
            docs = loader.load()
            
            # Ajouter les m√©tadonn√©es du fichier source
            for doc in docs:
                doc.metadata['source'] = file_path
                doc.metadata['file_type'] = 'txt'
            
            all_documents.extend(docs)
            stats["txt"] += 1
            print(f"  ‚úì {os.path.basename(file_path)}")
        except UnicodeDecodeError:
            # Si UTF-8 √©choue, essayer avec latin-1
            try:
                loader = TextLoader(file_path, encoding='latin-1')
                docs = loader.load()
                for doc in docs:
                    doc.metadata['source'] = file_path
                    doc.metadata['file_type'] = 'txt'
                all_documents.extend(docs)
                stats["txt"] += 1
                print(f"  ‚úì {os.path.basename(file_path)} (encodage latin-1)")
            except Exception as e:
                stats["errors"] += 1
                print(f"  ‚úó ERREUR {os.path.basename(file_path)}: {str(e)[:80]}")
        except Exception as e:
            stats["errors"] += 1
            print(f"  ‚úó ERREUR {os.path.basename(file_path)}: {str(e)[:80]}")
    
    print(f"\nüìà STATISTIQUES DE CHARGEMENT:")
    print(f"   ‚Ä¢ PDFs charg√©s : {stats['pdf']}/{len(pdf_files)}")
    print(f"   ‚Ä¢ TXTs charg√©s : {stats['txt']}/{len(txt_files)}")
    print(f"   ‚Ä¢ Erreurs : {stats['errors']}")
    print(f"   ‚Ä¢ Total de documents/pages charg√©s : {len(all_documents)}\n")
    
    return all_documents, stats

def ingest_documents():
    """
    Fonction principale : ex√©cute le pipeline complet d'ingestion RAG.
    """
    print("=" * 80)
    print("üöÄ D√âMARRAGE DU PIPELINE D'INGESTION RAG")
    print("=" * 80)
    print(f"üìÇ Source : {DATA_DIR}")
    print(f"üíæ Destination : {CHROMA_DB_DIR}")
    print(f"üè∑Ô∏è  Collection : {COLLECTION_NAME}")
    print(f"ü§ñ Mod√®le d'embedding : {EMBEDDING_MODEL}")
    print("=" * 80 + "\n")
    
    # 1. Nettoyage et pr√©paration
    clean_and_prepare_db()
    
    # 2. Chargement de tous les documents
    documents, stats = load_all_documents()
    if not documents:
        print("‚ùå Arr√™t de l'ingestion car aucun document n'a pu √™tre charg√©.")
        return

    # 3. D√©coupage (Chunking)
    print("‚úÇÔ∏è  D√©coupage des documents en chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(documents)
    print(f"‚úÖ D√©coupage termin√© : {len(chunks)} chunks cr√©√©s")
    print(f"   ‚Ä¢ Taille moyenne par chunk : {sum(len(c.page_content) for c in chunks) // len(chunks)} caract√®res\n")

    # 4. Vectorisation (Embeddings)
    print(f"üß† Initialisation du mod√®le d'embedding : {EMBEDDING_MODEL}")
    print("   (Premi√®re utilisation : t√©l√©chargement du mod√®le en cours...)")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    print("‚úÖ Mod√®le d'embedding pr√™t\n")

    # 5. Stockage dans ChromaDB
    print(f"üíæ Cr√©ation et persistance de la base de donn√©es vectorielle ChromaDB...")
    print(f"   Cela peut prendre quelques minutes pour {len(chunks)} chunks...\n")
    
    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_DB_DIR,
        collection_name=COLLECTION_NAME
    )
    
    print("\n" + "=" * 80)
    print("‚úÖ INGESTION TERMIN√âE AVEC SUCC√àS")
    print("=" * 80)
    print(f"üìä R√âSUM√â FINAL:")
    print(f"   ‚Ä¢ Documents sources : {stats['pdf'] + stats['txt']}")
    print(f"   ‚Ä¢ PDFs : {stats['pdf']}")
    print(f"   ‚Ä¢ TXTs : {stats['txt']}")
    print(f"   ‚Ä¢ Chunks vectoris√©s : {len(chunks)}")
    print(f"   ‚Ä¢ Base de donn√©es : {CHROMA_DB_DIR}")
    print(f"   ‚Ä¢ Collection : {COLLECTION_NAME}")
    print("=" * 80)
    print("\nüéâ Votre base de connaissances RAG est maintenant pr√™te √† l'emploi !")

# ==============================================================================
# 4. POINT D'ENTR√âE DU SCRIPT
# ==============================================================================

if __name__ == "__main__":
    ingest_documents()