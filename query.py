import os
import json
import time
from datetime import datetime
from typing import List, Dict
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# ==============================================================================
# 1. CONFIGURATION
# ==============================================================================

CHROMA_DB_DIR = "vectordb"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
LLM_MODEL = "tinyllama"
TOP_K_RETRIEVAL = 4  # Nombre de chunks √† r√©cup√©rer
RESULTS_FILE = "rag_evaluation_results.json"

# ==============================================================================
# 2. QUESTIONS DE TEST PAR CAT√âGORIE
# ==============================================================================

TEST_QUESTIONS = {
    "niveau_1_facile": [
        "Quelles sont les principales m√©thodes de pr√©vention contre les injections SQL ?",
        "Comment pr√©venir les attaques XSS (Cross-Site Scripting) ?",
        "Qu'est-ce que CSRF et comment s'en prot√©ger ?",
        "Quelles sont les bonnes pratiques pour le stockage des mots de passe ?",
        "Comment s√©curiser une API REST ?",
        "Qu'est-ce que la tactique Initial Access dans MITRE ATT&CK ?",
        "Quelles sont les techniques de Privilege Escalation ?",
        "Qu'est-ce qu'un SIEM et √† quoi sert-il ?",
        "Quelles sont les principales fonctionnalit√©s de Splunk ?",
        "Qu'est-ce qu'une r√®gle Sigma ?",
        "Comment fonctionne Microsoft Sentinel ?",
        "Qu'est-ce que la technique T1566 (Phishing) ?",
        "Comment d√©tecter un Lateral Movement ?",
        "Quelles sont les m√©thodes de Credential Access ?",
        "Comment utiliser Wazuh pour la d√©tection d'intrusion ?",
    ],
    "niveau_2_moyen": [
        "Comment mettre en place une architecture Zero Trust ?",
        "Quelles sont les bonnes pratiques pour s√©curiser Docker et Kubernetes ?",
        "Comment impl√©menter une strat√©gie DevSecOps efficace ?",
        "Quels sont les composants essentiels d'un SOC ?",
        "Comment centraliser et g√©rer efficacement les logs de s√©curit√© ?",
        "Quelles sont les √©tapes d'un processus de threat hunting ?",
        "Comment r√©pondre √† une attaque de ransomware ?",
        "Comment analyser une alerte de s√©curit√© dans un SIEM ?",
        "Quelles sont les bonnes pratiques de s√©curit√© pour AWS ?",
        "Comment s√©curiser un environnement Azure ?",
        "Quels sont les services de s√©curit√© de Google Cloud Platform ?",
        "Comment impl√©menter la s√©curit√© des conteneurs en production ?",
        "Quelles sont les phases d'un incident response ?",
        "Comment d√©tecter une compromission Active Directory ?",
        "Quelles diff√©rences entre IaaS, PaaS et SaaS en s√©curit√© ?",
    ],
    "niveau_3_difficile": [
        "Je d√©tecte une connexion SSH anormale √† 3h du matin depuis une IP √©trang√®re. Quelles sont les √©tapes d'investigation ?",
        "Comment d√©tecter et bloquer une attaque DDoS avec un SIEM ?",
        "Un utilisateur clique sur un lien de phishing. Quel est le playbook complet ?",
        "Comment investiguer une exfiltration de donn√©es via DNS tunneling ?",
        "Quels sont les indicateurs d'une attaque APT ?",
        "Quelle est la diff√©rence entre Splunk, Elastic SIEM et Microsoft Sentinel ?",
        "Diff√©rence entre r√®gles Sigma et YARA : quand utiliser l'une ou l'autre ?",
        "Quelle est la diff√©rence entre un WAF et un IDS/IPS ?",
        "Comment aligner une strat√©gie avec NIST CSF et ISO 27001 ?",
        "Quelles sont les exigences PCI-DSS pour la protection des donn√©es ?",
        "Comment mettre en conformit√© RGPD un syst√®me de logging ?",
        "Quelle est la diff√©rence entre CIS Controls et MITRE ATT&CK ?",
        "Comment mapper les contr√¥les aux tactiques MITRE ATT&CK ?",
        "Comment choisir entre SIEM on-premise ou cloud ?",
        "Avantages et inconv√©nients de Wazuh vs solutions commerciales ?",
    ],
    "niveau_4_avance": [
        "Comment concevoir une architecture de s√©curit√© multi-cloud avec d√©tection centralis√©e ?",
        "Quelle strat√©gie de d√©fense en profondeur pour microservices Kubernetes ?",
        "Comment impl√©menter une strat√©gie de threat intelligence avec un SIEM ?",
        "Comment automatiser la r√©ponse aux incidents avec SOAR ?",
        "Comment reconstruire la timeline d'une attaque depuis les logs Splunk ?",
        "Quels artefacts rechercher lors d'une investigation post-compromission Windows ?",
        "Comment d√©tecter une backdoor persistante dans un environnement Linux ?",
        "Comment corr√©ler des √©v√©nements entre plusieurs sources (SIEM, EDR, Firewall) ?",
        "Quelles techniques de Living off the Land sont difficiles √† d√©tecter ?",
        "Comment d√©tecter l'utilisation de Mimikatz ou Cobalt Strike ?",
        "Quelles techniques d'√©vasion de d√©tection sont les plus courantes ?",
        "Comment mettre en place un purple teaming efficace ?",
        "Quelles m√©triques pour mesurer la maturit√© d'un SOC ?",
        "Comment identifier un malware inconnu (zero-day) ?",
        "Quel plan de r√©silience contre une cyberattaque nation-state ?",
    ]
}

# ==============================================================================
# 3. MOTS-CL√âS ATTENDUS PAR QUESTION (Pour scoring automatique basique)
# ==============================================================================

EXPECTED_KEYWORDS = {
    "injection sql": ["prepared statements", "parameterized", "sanitize", "validation", "escape"],
    "xss": ["encode", "sanitize", "csp", "content security policy", "validate"],
    "csrf": ["token", "same-site", "cookie", "origin"],
    "mot de passe": ["hash", "bcrypt", "salt", "argon", "pbkdf"],
    "api rest": ["authentication", "authorization", "token", "rate limiting", "validation"],
    "initial access": ["phishing", "exploit", "valid accounts", "external remote"],
    "privilege escalation": ["exploit", "bypass", "sudo", "token", "abuse"],
    "siem": ["log", "correlation", "alert", "monitoring", "security"],
    "splunk": ["search", "spl", "index", "event", "correlation"],
    "sigma": ["detection", "rule", "yaml", "generic"],
}

# ==============================================================================
# 4. CLASSE D'√âVALUATION
# ==============================================================================

class RAGEvaluator:
    def __init__(self):
        self.rag_chain = None
        self.results = {
            "metadata": {
                "date": datetime.now().isoformat(),
                "model": LLM_MODEL,
                "embedding": EMBEDDING_MODEL,
                "top_k": TOP_K_RETRIEVAL
            },
            "categories": {},
            "global_stats": {}
        }
    
    def initialize_rag(self):
        """Initialise la cha√Æne RAG"""
        print("üîß Initialisation de la cha√Æne RAG...")
        
        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
        vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": TOP_K_RETRIEVAL})
        llm = Ollama(model=LLM_MODEL, temperature=0.1)
        
        template = """
Vous √™tes un assistant RAG expert en cybers√©curit√© et SecOps.
R√©pondez √† la question en vous basant UNIQUEMENT sur le contexte fourni.
Si le contexte ne contient pas la r√©ponse, dites "Je ne dispose pas d'informations suffisantes dans ma base de connaissances pour r√©pondre √† cette question."
Soyez pr√©cis, technique et concis.

Contexte: {context}

Question: {question}

R√©ponse:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        self.rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        print("‚úÖ Cha√Æne RAG initialis√©e\n")
    
    def evaluate_response(self, question: str, response: str) -> Dict:
        """√âvalue une r√©ponse basique par pr√©sence de mots-cl√©s"""
        score = 0
        max_score = 5
        
        response_lower = response.lower()
        
        # V√©rifier si c'est une r√©ponse "je ne sais pas"
        no_answer_phrases = [
            "je ne dispose pas",
            "pas d'information",
            "ne contient pas",
            "pas disponible",
            "cannot answer",
            "no information"
        ]
        
        is_no_answer = any(phrase in response_lower for phrase in no_answer_phrases)
        
        if is_no_answer:
            return {
                "score": 0,
                "max_score": max_score,
                "percentage": 0,
                "has_answer": False,
                "response_length": len(response),
                "keywords_found": []
            }
        
        # Scoring basique par longueur de r√©ponse
        response_length = len(response)
        if response_length > 200:
            score += 2
        elif response_length > 100:
            score += 1
        
        # Recherche de mots-cl√©s pertinents
        keywords_found = []
        for keyword_group, keywords in EXPECTED_KEYWORDS.items():
            if any(kw in question.lower() for kw in keyword_group.split()):
                for kw in keywords:
                    if kw in response_lower:
                        keywords_found.append(kw)
                        score += 0.5
        
        # V√©rifier si la r√©ponse semble structur√©e
        if any(marker in response for marker in ["1.", "2.", "-", "‚Ä¢", "*"]):
            score += 1
        
        score = min(score, max_score)
        
        return {
            "score": round(score, 2),
            "max_score": max_score,
            "percentage": round((score / max_score) * 100, 1),
            "has_answer": True,
            "response_length": response_length,
            "keywords_found": keywords_found
        }
    
    def test_question(self, question: str, category: str, question_num: int) -> Dict:
        """Teste une question et retourne les r√©sultats"""
        print(f"  [{question_num}] Testing: {question[:60]}...")
        
        start_time = time.time()
        
        try:
            response = self.rag_chain.invoke(question)
            elapsed_time = time.time() - start_time
            
            evaluation = self.evaluate_response(question, response)
            
            result = {
                "question": question,
                "response": response,
                "evaluation": evaluation,
                "time_seconds": round(elapsed_time, 2),
                "success": True,
                "error": None
            }
            
            status = "‚úÖ" if evaluation["percentage"] >= 60 else "‚ö†Ô∏è" if evaluation["percentage"] >= 30 else "‚ùå"
            print(f"    {status} Score: {evaluation['percentage']}% | Time: {elapsed_time:.2f}s")
            
        except Exception as e:
            result = {
                "question": question,
                "response": None,
                "evaluation": {"score": 0, "max_score": 5, "percentage": 0},
                "time_seconds": 0,
                "success": False,
                "error": str(e)
            }
            print(f"    ‚ùå ERROR: {str(e)[:50]}")
        
        return result
    
    def run_evaluation(self):
        """Ex√©cute l'√©valuation compl√®te"""
        print("="*80)
        print("üöÄ D√âMARRAGE DE L'√âVALUATION RAG SECOPS")
        print("="*80)
        print(f"üìÖ Date: {self.results['metadata']['date']}")
        print(f"ü§ñ Mod√®le LLM: {LLM_MODEL}")
        print(f"üß† Mod√®le Embedding: {EMBEDDING_MODEL}")
        print(f"üìä Top K Retrieval: {TOP_K_RETRIEVAL}")
        print("="*80 + "\n")
        
        self.initialize_rag()
        
        total_questions = sum(len(questions) for questions in TEST_QUESTIONS.values())
        current_question = 0
        
        for category, questions in TEST_QUESTIONS.items():
            print(f"\n{'='*80}")
            print(f"üìÇ CAT√âGORIE: {category.upper().replace('_', ' ')}")
            print(f"{'='*80}")
            
            category_results = []
            
            for i, question in enumerate(questions, 1):
                current_question += 1
                result = self.test_question(question, category, current_question)
                category_results.append(result)
                
                # Petite pause pour ne pas surcharger
                time.sleep(0.5)
            
            # Calcul des stats de la cat√©gorie
            successful_tests = [r for r in category_results if r["success"]]
            
            if successful_tests:
                avg_score = sum(r["evaluation"]["percentage"] for r in successful_tests) / len(successful_tests)
                avg_time = sum(r["time_seconds"] for r in successful_tests) / len(successful_tests)
                answers_provided = sum(1 for r in successful_tests if r["evaluation"]["has_answer"])
                
                category_stats = {
                    "total_questions": len(questions),
                    "successful_tests": len(successful_tests),
                    "failed_tests": len(questions) - len(successful_tests),
                    "average_score": round(avg_score, 2),
                    "average_time": round(avg_time, 2),
                    "answers_provided": answers_provided,
                    "no_answer_count": len(successful_tests) - answers_provided
                }
            else:
                category_stats = {
                    "total_questions": len(questions),
                    "successful_tests": 0,
                    "failed_tests": len(questions),
                    "average_score": 0,
                    "average_time": 0,
                    "answers_provided": 0,
                    "no_answer_count": 0
                }
            
            self.results["categories"][category] = {
                "stats": category_stats,
                "questions": category_results
            }
            
            print(f"\nüìä Stats {category}:")
            print(f"   Score moyen: {category_stats['average_score']:.1f}%")
            print(f"   R√©ponses fournies: {category_stats['answers_provided']}/{category_stats['total_questions']}")
            print(f"   Temps moyen: {category_stats['average_time']:.2f}s")
        
        # Calcul des statistiques globales
        self.calculate_global_stats()
        self.print_final_report()
        self.save_results()
    
    def calculate_global_stats(self):
        """Calcule les statistiques globales"""
        all_successful = []
        total_questions = 0
        
        for category_data in self.results["categories"].values():
            total_questions += category_data["stats"]["total_questions"]
            all_successful.extend([
                q for q in category_data["questions"] if q["success"]
            ])
        
        if all_successful:
            global_avg_score = sum(q["evaluation"]["percentage"] for q in all_successful) / len(all_successful)
            global_avg_time = sum(q["time_seconds"] for q in all_successful) / len(all_successful)
            total_answers = sum(1 for q in all_successful if q["evaluation"]["has_answer"])
            
            # Distribution des scores
            excellent = sum(1 for q in all_successful if q["evaluation"]["percentage"] >= 80)
            good = sum(1 for q in all_successful if 60 <= q["evaluation"]["percentage"] < 80)
            average = sum(1 for q in all_successful if 40 <= q["evaluation"]["percentage"] < 60)
            poor = sum(1 for q in all_successful if q["evaluation"]["percentage"] < 40)
            
            self.results["global_stats"] = {
                "total_questions": total_questions,
                "successful_tests": len(all_successful),
                "failed_tests": total_questions - len(all_successful),
                "global_average_score": round(global_avg_score, 2),
                "global_average_time": round(global_avg_time, 2),
                "total_answers_provided": total_answers,
                "no_answer_total": len(all_successful) - total_answers,
                "score_distribution": {
                    "excellent_80_100": excellent,
                    "good_60_79": good,
                    "average_40_59": average,
                    "poor_0_39": poor
                }
            }
    
    def print_final_report(self):
        """Affiche le rapport final"""
        print("\n" + "="*80)
        print("üìä RAPPORT FINAL D'√âVALUATION RAG")
        print("="*80)
        
        stats = self.results["global_stats"]
        
        print(f"\nüéØ R√âSULTATS GLOBAUX:")
        print(f"   Questions test√©es: {stats['total_questions']}")
        print(f"   Tests r√©ussis: {stats['successful_tests']}")
        print(f"   Tests √©chou√©s: {stats['failed_tests']}")
        print(f"   Score moyen global: {stats['global_average_score']:.1f}%")
        print(f"   Temps de r√©ponse moyen: {stats['global_average_time']:.2f}s")
        print(f"   R√©ponses fournies: {stats['total_answers_provided']}/{stats['successful_tests']}")
        
        print(f"\nüìà DISTRIBUTION DES SCORES:")
        dist = stats['score_distribution']
        print(f"   üåü Excellent (80-100%): {dist['excellent_80_100']}")
        print(f"   üëç Bon (60-79%): {dist['good_60_79']}")
        print(f"   üòê Moyen (40-59%): {dist['average_40_59']}")
        print(f"   ‚ùå Faible (0-39%): {dist['poor_0_39']}")
        
        print(f"\nüìä PERFORMANCE PAR CAT√âGORIE:")
        for category, data in self.results["categories"].items():
            cat_stats = data["stats"]
            print(f"   ‚Ä¢ {category.replace('_', ' ').title():30s}: {cat_stats['average_score']:5.1f}%")
        
        # Interpr√©tation
        print(f"\nüí° INTERPR√âTATION:")
        if stats['global_average_score'] >= 70:
            print("   ‚úÖ Excellent ! Votre RAG fonctionne tr√®s bien.")
        elif stats['global_average_score'] >= 50:
            print("   üëç Bon r√©sultat. Quelques am√©liorations possibles.")
        elif stats['global_average_score'] >= 30:
            print("   ‚ö†Ô∏è  R√©sultats moyens. V√©rifiez le retrieval et les chunks.")
        else:
            print("   ‚ùå R√©sultats faibles. Probl√®me de retrieval ou de documents.")
        
        print("\n" + "="*80)
    
    def save_results(self):
        """Sauvegarde les r√©sultats dans un fichier JSON"""
        with open(RESULTS_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ R√©sultats sauvegard√©s dans: {RESULTS_FILE}")
        print(f"üìÅ Vous pouvez analyser les d√©tails dans ce fichier.\n")

# ==============================================================================
# 5. POINT D'ENTR√âE
# ==============================================================================

def main():
    print("\n" + "="*80)
    print("üîç √âVALUATION AUTOMATIQUE DU RAG SECOPS")
    print("="*80)
    print("Ce script va tester votre RAG sur 60 questions r√©parties en 4 niveaux.")
    print("Cela prendra environ 5-10 minutes selon votre machine.")
    print("="*80 + "\n")
    
    try:
        evaluator = RAGEvaluator()
        evaluator.run_evaluation()
        
        print("\n‚úÖ √âvaluation termin√©e avec succ√®s !")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  √âvaluation interrompue par l'utilisateur.")
    except Exception as e:
        print(f"\n\n‚ùå Erreur fatale: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()